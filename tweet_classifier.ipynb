{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/filippofocaccia/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from nltk.corpus import stopwords\n",
    "from langdetect import detect\n",
    "import re\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from langdetect import detect\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "import os\n",
    "import openai\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary with file paths and metadata\n",
    "politicians_data = {\n",
    "    'FdI_Meloni': {'file': 'politicians_data/GiorgiaMeloni_tweets_combined.csv', 'politician': 'Meloni', 'party': 'FdI'},\n",
    "    'FdI_LaRussa': {'file': 'politicians_data/IgnazioLaRussa_tweets_combined.csv', 'politician': 'LaRussa', 'party': 'FdI'},\n",
    "    'FI_Berlusconi': {'file': 'politicians_data/SilvioBerlusconi_tweets_combined.csv', 'politician': 'Berlusconi', 'party': 'FI'},\n",
    "    'FI_Tajani': {'file': 'politicians_data/AntonioTajani_tweets_combined.csv', 'politician': 'Tajani', 'party': 'FI'},\n",
    "    'Lega_Salvini': {'file': 'politicians_data/MatteoSalvini_tweets_combined.csv', 'politician': 'Salvini', 'party': 'Lega'},\n",
    "    'M5S_DiMaio': {'file': 'politicians_data/luigidimaio_tweets_combined.csv', 'politician': 'Di Maio', 'party': 'M5S'},\n",
    "    'M5S_Conte': {'file': 'politicians_data/GiuseppeConte_tweets_combined.csv', 'politician': 'Conte', 'party': 'M5S'},\n",
    "    'Az_Calenda': {'file': 'politicians_data/CarloCalenda_tweets_combined.csv', 'politician': 'Calenda', 'party': 'Azione'},\n",
    "    'IV_Renzi': {'file': 'politicians_data/MatteoRenzi_tweets_combined.csv', 'politician': 'Renzi', 'party': 'IV'},\n",
    "    'PEeur_Bonino': {'file': 'politicians_data/emmabonino_tweets_combined.csv', 'politician': 'Bonino', 'party': 'PEeur'},\n",
    "    'PD_Shlein': {'file': 'politicians_data/EllySchlein_tweets_combined.csv', 'politician': 'Schlein', 'party': 'PD'},\n",
    "    'PD_Letta': {'file': 'politicians_data/EnricoLetta_tweets_combined.csv', 'politician': 'Letta', 'party': 'PD'},\n",
    "    'EV_Fratoianni': {'file': 'politicians_data/NicolaFratoianni_tweets_combined.csv', 'politician': 'Fratoianni', 'party': 'EV'},\n",
    "    'NcI_Lupi': {'file': 'politicians_data/MaurizioLupi_tweets_combined.csv', 'politician': 'Lupi', 'party': 'NcI'}\n",
    "}\n",
    "\n",
    "# Read and process each file\n",
    "politicians_list = []\n",
    "for key, data in politicians_data.items():\n",
    "    df = pd.read_csv(data['file'])\n",
    "    df['politician'] = data['politician']\n",
    "    df['party'] = data['party']\n",
    "    politicians_list.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "politicians = pd.concat(politicians_list, ignore_index=True)\n",
    "\n",
    "# Load the users dataset\n",
    "users = pd.read_csv('train_data/user_tweets.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'left', 1: 'moderate_left', 2: 'moderate_right', 3: 'right'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how can i see the different values in the ideology_multiclass column?\n",
    "users['ideology_multiclass']= users['ideology_multiclass'].astype('category')\n",
    "dict(enumerate(users['ideology_multiclass'].cat.categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let me delete the urls column and the retweet column\n",
    "politicians = politicians.drop(columns=[politicians.columns[2], politicians.columns[5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ideology_num\n",
       "1.0    11638\n",
       "3.0     7882\n",
       "0.0     4575\n",
       "2.0     3320\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#i want to use this map now to also add the ideology_multiclass column to the politicians dataframe\n",
    "politicians_parties = {\n",
    "    'Meloni': 3,  # 'right'\n",
    "    'Salvini': 3,  # 'right'\n",
    "    'Berlusconi': 2,  # 'moderate_right'\n",
    "    'Letta': 0,  # 'moderate_left'\n",
    "    'Conte': 1,  # 'moderate_left'\n",
    "    'Calenda': 1,  # 'moderate_left'\n",
    "    'Renzi': 1,  # 'moderate_left'\n",
    "    'Bonino': 0,  # 'left'\n",
    "    'Di Maio': 1,  # 'moderate_left'\n",
    "    'Fratoianni': 0,  # 'left'\n",
    "    'Bonelli': 0,  # 'left'\n",
    "    'Lupi': 2,  # 'moderate_right'\n",
    "    'La Russa': 3,  # 'right'\n",
    "    'Tajani': 2,  # 'moderate_right'\n",
    "    'Schlein': 0   # 'left'\n",
    "}\n",
    "politicians['ideology_num'] = politicians['politician'].map(politicians_parties)\n",
    "politicians['ideology_num'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "politicians['ideology_num'] = politicians['ideology_num'].apply(lambda x: 3 if x in (2.0, 3.0) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "assert enc.decode(enc.encode(\"hello world\")) == \"hello world\"\n",
    "#gpt 4o rate limits\n",
    "# 30 000 tokens per minute\n",
    "# 500 requests per minute\n",
    "# To get the tokeniser corresponding to a specific model in the OpenAI API:\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "def merge_tweets(df):\n",
    "    prompts = []\n",
    "    current_batch = []\n",
    "    current_text = \"\"\n",
    "    n_tokens = 0\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        tweet = row['Content']\n",
    "        tweet_text = str(index) + \": \" + tweet + \"\\n\"\n",
    "        tweet_tokens = len(enc.encode(tweet_text))\n",
    "        \n",
    "        # If this tweet would exceed token limit, save batch and start new one\n",
    "        if n_tokens + tweet_tokens > 29000:\n",
    "            prompts.append({\n",
    "                'text': current_text,\n",
    "                'indices': current_batch\n",
    "            })\n",
    "            current_text = tweet_text\n",
    "            current_batch = [index]\n",
    "            n_tokens = tweet_tokens\n",
    "        else:\n",
    "            # Add to current batch\n",
    "            current_text += tweet_text\n",
    "            current_batch.append(index)\n",
    "            n_tokens += tweet_tokens\n",
    "    \n",
    "    # Add the last batch\n",
    "    if current_text:\n",
    "        prompts.append({\n",
    "            'text': current_text,\n",
    "            'indices': current_batch\n",
    "        })\n",
    "        \n",
    "    return prompts\n",
    "\n",
    "\n",
    "# meloni_sample = politicians[politicians['politician'] == 'Meloni'].head(1000).copy()\n",
    "# merged_dic= merge_tweets(meloni_sample)\n",
    "# # Save the merged list to a file\n",
    "# with open('merged_tweets.txt', 'w') as f:\n",
    "#     for item in merged_dic:\n",
    "#         f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load environment variables from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# Set the OpenAI API key from environment variable\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "def classify_tweet_batches(prompts):\n",
    "    all_results = {}\n",
    "    \n",
    "    for prompt_data in prompts:\n",
    "        prompt_text = prompt_data['text']\n",
    "        indices = prompt_data['indices']  # Now we'll use this\n",
    "        \n",
    "        # Call the OpenAI API\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"\"\"\n",
    "                You are an expert Italian political content analyst. I will give you a batch of tweets from Italian politicians.\n",
    "                For EACH tweet, determine if it contains significant political content or is just promotional content.\n",
    "                \n",
    "                Significant tweets include:\n",
    "                - Policy positions or proposals\n",
    "                - Political criticism of opponents\n",
    "                - Commentary on current events\n",
    "                - Substantive discussions of issues\n",
    "                \n",
    "                Non-significant (broadcast) tweets include:\n",
    "                - TV/radio appearance announcements\n",
    "                - Live stream announcements\n",
    "                - Schedule announcements\n",
    "                - Simple greetings without political substance\n",
    "                \n",
    "                Respond with ONLY the tweet index number followed by either SIGNIFICANT or BROADCAST, one per line.\n",
    "                Example format:\n",
    "                123: SIGNIFICANT\n",
    "                124: BROADCAST\n",
    "                125: SIGNIFICANT\n",
    "                \"\"\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Tweets:\\n{prompt_text}\"}\n",
    "            ],\n",
    "            temperature=0.0,\n",
    "            max_tokens=3000\n",
    "        )\n",
    "        \n",
    "        result = response.choices[0].message.content.strip()\n",
    "        \n",
    "        # Parse the results and match with expected indices\n",
    "        result_lines = result.split('\\n')\n",
    "        \n",
    "        # Ensure we have a result for each index in this batch\n",
    "        for i, idx in enumerate(indices):\n",
    "            # Try to find a result for this index\n",
    "            result_found = False\n",
    "            for line in result_lines:\n",
    "                if line.startswith(f\"{idx}:\") or line.startswith(f\"{idx} :\"):\n",
    "                    parts = line.split(':', 1)\n",
    "                    classification = \"SIGNIFICANT\" if \"SIGNIFICANT\" in parts[1].upper() else \"BROADCAST\"\n",
    "                    all_results[idx] = classification\n",
    "                    result_found = True\n",
    "                    break\n",
    "            \n",
    "            # If no result was found for this index, assign a default\n",
    "            if not result_found:\n",
    "                all_results[idx] = \"UNKNOWN\"\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "politicians_merged= merge_tweets(politicians)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75190aed408f40ee85f62115fff5e11e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing tweets:   0%|          | 0/59 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting classification...\n",
      "Rate limit reached. Waiting for 60 seconds...\n",
      "Rate limit reached. Waiting for 60 seconds...\n",
      "Rate limit reached. Waiting for 60 seconds...\n",
      "Rate limit reached. Waiting for 60 seconds...\n",
      "Rate limit reached. Waiting for 60 seconds...\n",
      "Rate limit reached. Waiting for 60 seconds...\n",
      "Rate limit reached. Waiting for 60 seconds...\n",
      "Rate limit reached. Waiting for 60 seconds...\n",
      "Classification completed.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "tweet_tokens = 0\n",
    "results = {}\n",
    "progress_bar = tqdm(politicians_merged, desc=\"Processing tweets\", unit=\"batch\")\n",
    "print(\"Starting classification...\")\n",
    "for tweets in progress_bar:\n",
    "    # Calculate token count for the current batch\n",
    "    tweet_tokens += len(enc.encode(tweets['text']))\n",
    "    \n",
    "    # Classify the tweets in the current batch\n",
    "    batch_results = classify_tweet_batches([tweets])\n",
    "    results.update(batch_results)\n",
    "    \n",
    "    # Update the progress bar description\n",
    "    progress_bar.set_postfix(tokens_used=tweet_tokens, results_collected=len(results))\n",
    "    \n",
    "    # Handle rate limiting\n",
    "    if tweet_tokens > 200000:\n",
    "        print(\"Rate limit reached. Waiting for 60 seconds...\")\n",
    "        time.sleep(60)\n",
    "        tweet_tokens = 0\n",
    "print(\"Classification completed.\")\n",
    "# Add classifications to the DataFrame\n",
    "politicians['classification'] = results.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "politicians['classification'] = politicians['classification'].replace({'SIGNIFICANT': 1, 'BROADCAST': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "classification\n",
       "1    17245\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "politicians['classification'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "ID",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Content",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Likes",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "politician",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "party",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "ideology_num",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "classification",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "693fd358-6727-4818-a638-dce083281fa7",
       "rows": [],
       "shape": {
        "columns": 8,
        "rows": 0
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>ID</th>\n",
       "      <th>Content</th>\n",
       "      <th>Likes</th>\n",
       "      <th>politician</th>\n",
       "      <th>party</th>\n",
       "      <th>ideology_num</th>\n",
       "      <th>classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Date, ID, Content, Likes, politician, party, ideology_num, classification]\n",
       "Index: []"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "politicians[politicians['classification'] == 'UNKNOWN'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the rows with classification 0\n",
    "politicians = politicians[politicians['classification'].isin([1, 'UNKNOWN'])]\n",
    "# save the new dataset\n",
    "politicians.to_csv('politicians_data/politicians_classified.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
