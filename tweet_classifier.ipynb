{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook aims to filter the original dataset using OpenAI API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import openai\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "politicians_data = {\n",
    "    'FdI_Meloni': {'file': 'politicians_data/GiorgiaMeloni_tweets_combined.csv', 'politician': 'Meloni', 'party': 'FdI'},\n",
    "    'FdI_LaRussa': {'file': 'politicians_data/IgnazioLaRussa_tweets_combined.csv', 'politician': 'LaRussa', 'party': 'FdI'},\n",
    "    'FI_Berlusconi': {'file': 'politicians_data/SilvioBerlusconi_tweets_combined.csv', 'politician': 'Berlusconi', 'party': 'FI'},\n",
    "    'FI_Tajani': {'file': 'politicians_data/AntonioTajani_tweets_combined.csv', 'politician': 'Tajani', 'party': 'FI'},\n",
    "    'Lega_Salvini': {'file': 'politicians_data/MatteoSalvini_tweets_combined.csv', 'politician': 'Salvini', 'party': 'Lega'},\n",
    "    'M5S_DiMaio': {'file': 'politicians_data/luigidimaio_tweets_combined.csv', 'politician': 'Di Maio', 'party': 'M5S'},\n",
    "    'M5S_Conte': {'file': 'politicians_data/GiuseppeConte_tweets_combined.csv', 'politician': 'Conte', 'party': 'M5S'},\n",
    "    'Az_Calenda': {'file': 'politicians_data/CarloCalenda_tweets_combined.csv', 'politician': 'Calenda', 'party': 'Azione'},\n",
    "    'IV_Renzi': {'file': 'politicians_data/MatteoRenzi_tweets_combined.csv', 'politician': 'Renzi', 'party': 'IV'},\n",
    "    'PEeur_Bonino': {'file': 'politicians_data/emmabonino_tweets_combined.csv', 'politician': 'Bonino', 'party': 'PEeur'},\n",
    "    'PD_Shlein': {'file': 'politicians_data/EllySchlein_tweets_combined.csv', 'politician': 'Schlein', 'party': 'PD'},\n",
    "    'PD_Letta': {'file': 'politicians_data/EnricoLetta_tweets_combined.csv', 'politician': 'Letta', 'party': 'PD'},\n",
    "    'EV_Fratoianni': {'file': 'politicians_data/NicolaFratoianni_tweets_combined.csv', 'politician': 'Fratoianni', 'party': 'EV'},\n",
    "    'NcI_Lupi': {'file': 'politicians_data/MaurizioLupi_tweets_combined.csv', 'politician': 'Lupi', 'party': 'NcI'}\n",
    "}\n",
    "\n",
    "politicians_list = []\n",
    "for key, data in politicians_data.items():\n",
    "    df = pd.read_csv(data['file'])\n",
    "    df['politician'] = data['politician']\n",
    "    df['party'] = data['party']\n",
    "    politicians_list.append(df)\n",
    "\n",
    "politicians = pd.concat(politicians_list, ignore_index=True)\n",
    "\n",
    "users = pd.read_csv('train_data/user_tweets.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'left', 1: 'moderate_left', 2: 'moderate_right', 3: 'right'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users['ideology_multiclass']= users['ideology_multiclass'].astype('category')\n",
    "dict(enumerate(users['ideology_multiclass'].cat.categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "politicians = politicians.drop(columns=[politicians.columns[2], politicians.columns[5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ideology_num\n",
       "1.0    11638\n",
       "3.0     7882\n",
       "0.0     4575\n",
       "2.0     3320\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "politicians_parties = {\n",
    "    'Meloni': 3,  # 'right'\n",
    "    'Salvini': 3,  # 'right'\n",
    "    'Berlusconi': 2,  # 'moderate_right'\n",
    "    'Letta': 0,  # 'moderate_left'\n",
    "    'Conte': 1,  # 'moderate_left'\n",
    "    'Calenda': 1,  # 'moderate_left'\n",
    "    'Renzi': 1,  # 'moderate_left'\n",
    "    'Bonino': 0,  # 'left'\n",
    "    'Di Maio': 1,  # 'moderate_left'\n",
    "    'Fratoianni': 0,  # 'left'\n",
    "    'Bonelli': 0,  # 'left'\n",
    "    'Lupi': 2,  # 'moderate_right'\n",
    "    'La Russa': 3,  # 'right'\n",
    "    'Tajani': 2,  # 'moderate_right'\n",
    "    'Schlein': 0   # 'left'\n",
    "}\n",
    "politicians['ideology_num'] = politicians['politician'].map(politicians_parties)\n",
    "politicians['ideology_num'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "politicians['ideology_num'] = politicians['ideology_num'].apply(lambda x: 3 if x in (2.0, 3.0) else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We now filter the tweets using the api keeping in mind the rate limits are the following: 30 000 tokens per minute and 500 requests per minute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To avoid the request per day limit we use this function to create bathes merging datasets while computing tokens to ensure to remain under the rate limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "assert enc.decode(enc.encode(\"hello world\")) == \"hello world\"\n",
    "\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "def merge_tweets(df):\n",
    "    prompts = []\n",
    "    current_batch = []\n",
    "    current_text = \"\"\n",
    "    n_tokens = 0\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        tweet = row['Content']\n",
    "        tweet_text = str(index) + \": \" + tweet + \"\\n\"\n",
    "        tweet_tokens = len(enc.encode(tweet_text))\n",
    "        \n",
    "        if n_tokens + tweet_tokens > 29000:\n",
    "            prompts.append({\n",
    "                'text': current_text,\n",
    "                'indices': current_batch\n",
    "            })\n",
    "            current_text = tweet_text\n",
    "            current_batch = [index]\n",
    "            n_tokens = tweet_tokens\n",
    "        else:\n",
    "            current_text += tweet_text\n",
    "            current_batch.append(index)\n",
    "            n_tokens += tweet_tokens\n",
    "    \n",
    "    if current_text:\n",
    "        prompts.append({\n",
    "            'text': current_text,\n",
    "            'indices': current_batch\n",
    "        })\n",
    "        \n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can classify the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "def classify_tweet_batches(prompts):\n",
    "    all_results = {}\n",
    "    \n",
    "    for prompt_data in prompts:\n",
    "        prompt_text = prompt_data['text']\n",
    "        indices = prompt_data['indices']  # Now we'll use this\n",
    "        \n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"\"\"\n",
    "                You are an expert Italian political content analyst. I will give you a batch of tweets from Italian politicians.\n",
    "                For EACH tweet, determine if it contains significant political content or is just promotional content.\n",
    "                \n",
    "                Significant tweets include:\n",
    "                - Policy positions or proposals\n",
    "                - Political criticism of opponents\n",
    "                - Commentary on current events\n",
    "                - Substantive discussions of issues\n",
    "                \n",
    "                Non-significant (broadcast) tweets include:\n",
    "                - TV/radio appearance announcements\n",
    "                - Live stream announcements\n",
    "                - Schedule announcements\n",
    "                - Simple greetings without political substance\n",
    "                \n",
    "                Respond with ONLY the tweet index number followed by either SIGNIFICANT or BROADCAST, one per line.\n",
    "                Example format:\n",
    "                123: SIGNIFICANT\n",
    "                124: BROADCAST\n",
    "                125: SIGNIFICANT\n",
    "                \"\"\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Tweets:\\n{prompt_text}\"}\n",
    "            ],\n",
    "            temperature=0.0,\n",
    "            max_tokens=3000\n",
    "        )\n",
    "        \n",
    "        result = response.choices[0].message.content.strip()\n",
    "        \n",
    "        result_lines = result.split('\\n')\n",
    "        \n",
    "        for i, idx in enumerate(indices):\n",
    "            result_found = False\n",
    "            for line in result_lines:\n",
    "                if line.startswith(f\"{idx}:\") or line.startswith(f\"{idx} :\"):\n",
    "                    parts = line.split(':', 1)\n",
    "                    classification = \"SIGNIFICANT\" if \"SIGNIFICANT\" in parts[1].upper() else \"BROADCAST\"\n",
    "                    all_results[idx] = classification\n",
    "                    result_found = True\n",
    "                    break\n",
    "            \n",
    "            if not result_found:\n",
    "                all_results[idx] = \"UNKNOWN\"\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "politicians_merged= merge_tweets(politicians)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokens = 0\n",
    "results = {}\n",
    "progress_bar = tqdm(politicians_merged, desc=\"Processing tweets\", unit=\"batch\")\n",
    "print(\"Starting classification...\")\n",
    "for tweets in progress_bar:\n",
    "    tweet_tokens += len(enc.encode(tweets['text']))\n",
    "    \n",
    "    batch_results = classify_tweet_batches([tweets])\n",
    "    results.update(batch_results)\n",
    "    \n",
    "    progress_bar.set_postfix(tokens_used=tweet_tokens, results_collected=len(results))\n",
    "    \n",
    "    if tweet_tokens > 200000:\n",
    "        print(\"Rate limit reached. Waiting for 60 seconds...\")\n",
    "        time.sleep(60)\n",
    "        tweet_tokens = 0\n",
    "print(\"Classification completed.\")\n",
    "politicians['classification'] = results.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "politicians['classification'] = politicians['classification'].replace({'SIGNIFICANT': 1, 'BROADCAST': 0})\n",
    "politicians = politicians[politicians['classification'].isin([1, 'UNKNOWN'])]\n",
    "politicians.to_csv('politicians_data/politicians_classified.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
