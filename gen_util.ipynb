{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook performs dataset preprocessing and standardization on the generated datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from langdetect import detect\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from langdetect import detect\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline= pd.read_csv('gen/baseline2.csv')\n",
    "finetuned= pd.read_csv('gen/finetuned2.csv')\n",
    "right_topics= pd.read_csv('politicians_data/topics_right.csv')\n",
    "left_topics= pd.read_csv('politicians_data/topics_left.csv')\n",
    "baseline_b = pd.read_csv('gen/baseline_b.csv')\n",
    "finetuned_b = pd.read_csv('gen/finetuned_b.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_topics(topics):\n",
    "    \n",
    "    new_topics=[]\n",
    "    for topic in topics:\n",
    "        topic_tokens= topic.split()\n",
    "        if 'Contenuto:' in topic_tokens:\n",
    "            index = topic_tokens.index('Contenuto:')\n",
    "            topic_tokens[index]= ''\n",
    "            topic= ' '.join(topic_tokens)\n",
    "            topic=topic.strip()\n",
    "            new_topics.append(topic)\n",
    "        elif 'Argomento:' in topic_tokens:\n",
    "            index = topic_tokens.index('Argomento:')\n",
    "            topic_tokens[index]= ''\n",
    "            topic= ' '.join(topic_tokens)\n",
    "            topic=topic.strip()\n",
    "\n",
    "            new_topics.append(topic)\n",
    "        else:\n",
    "            new_topics.append(topic)\n",
    "    return new_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_party(parties):\n",
    "    new_parties=[]\n",
    "    for party in parties:\n",
    "        party_tokens= party.split()\n",
    "        if ' '.join(party_tokens[:2])=='Partito politico:':\n",
    "            party_tokens[:2]= ''\n",
    "            party= ' '.join(party_tokens)\n",
    "            party=party.strip()\n",
    "            new_parties.append(party)\n",
    "        elif ' '.join(party_tokens[:3])=='Partito di appartenenza:':\n",
    "            party_tokens[:3]= ''\n",
    "            party= ' '.join(party_tokens)\n",
    "            party=party.strip()\n",
    "            new_parties.append(party)\n",
    "        elif ' '.join(party_tokens[:2])=='Ideologia politicaPartito:':\n",
    "            party_tokens[:2]= ''\n",
    "            party= ' '.join(party_tokens)\n",
    "            party=party.strip()\n",
    "            new_parties.append(party)\n",
    "        else:\n",
    "            new_parties.append(party)           \n",
    "    return new_parties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentiment(sentiments):\n",
    "    new_sentiments=[]\n",
    "    for sentiment in sentiments:\n",
    "        sentiment_tokens= sentiment.split()\n",
    "        if 'Accezione:' in sentiment_tokens:\n",
    "            sentiment_tokens[0]= ''\n",
    "            if 'Neutrale' in sentiment_tokens:\n",
    "                index = sentiment_tokens.index('Neutrale')\n",
    "                sentiment_tokens[index]= 'Neutro'\n",
    "            sentiment= ' '.join(sentiment_tokens)\n",
    "            sentiment=sentiment.strip()\n",
    "            new_sentiments.append(sentiment)\n",
    "        elif 'Neutrale' in sentiment_tokens:\n",
    "                index = sentiment_tokens.index('Neutrale')\n",
    "                sentiment_tokens[index]= 'Neutro'\n",
    "                sentiment= ' '.join(sentiment_tokens)\n",
    "                sentiment=sentiment.strip()\n",
    "                new_sentiments.append(sentiment)\n",
    "        else:\n",
    "            new_sentiments.append(sentiment)           \n",
    "    return new_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_macro_topic(df, macro_topics, flag=False): \n",
    "    Macros=[]\n",
    "    for index, row in df.iterrows():\n",
    "        topic= row['topic']\n",
    "        #remove any extra spaces at the beginning and end of the topic\n",
    "        macro_topic= macro_topics[macro_topics['TopicName'] == topic]\n",
    "        macro_topic= macro_topic['MacroTopic'].values[0]\n",
    "        Macros.append(macro_topic)\n",
    "    return Macros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/filippofocaccia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/filippofocaccia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/filippofocaccia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words_italian = set(stopwords.words('italian'))\n",
    "stop_words_english = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "nlp_it = spacy.load('it_core_news_sm')\n",
    "nlp_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    try:\n",
    "        # Detect language\n",
    "        lang = detect(text)\n",
    "    except:\n",
    "        lang = 'unknown'\n",
    "    \n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\@\\w+|\\#', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    if lang == 'it':\n",
    "        doc = nlp_it(text.lower())\n",
    "        true_tokens = [str(token) for token in doc \n",
    "                      if token.text not in stop_words_italian and len(token.text) > 2 ]\n",
    "    \n",
    "    elif lang == 'en':\n",
    "        doc = nlp_en(text.lower())\n",
    "        true_tokens = [str(token) for token in doc \n",
    "                      if token.text not in stop_words_english and len(token.text) > 2 ]\n",
    "    \n",
    "    else: \n",
    "        tokens = word_tokenize(text.lower())\n",
    "        true_tokens = [str(word) for word in tokens if len(word) > 2]\n",
    "    \n",
    "    return true_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's first clean the topics column and standardise evrything in the baseline and on the finetuned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_old_topics= baseline['topic']\n",
    "baseline_b_old_topics= baseline_b['topic']\n",
    "\n",
    "finetuned_old_topics= finetuned['topic']\n",
    "finetuned_b_old_topics= finetuned_b['topic']\n",
    "\n",
    "new_base_topics= clean_topics(base_old_topics)\n",
    "new_finetuned_topics= clean_topics(finetuned_old_topics)\n",
    "new_base_topics_b= clean_topics(baseline_b_old_topics)\n",
    "new_finetuned_topics_b= clean_topics(finetuned_b_old_topics)\n",
    "\n",
    "baseline['topic']= new_base_topics\n",
    "finetuned['topic']= new_finetuned_topics\n",
    "baseline_b['topic']= new_base_topics_b\n",
    "finetuned_b['topic']= new_finetuned_topics_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We then create a dataframe for all the macrotopics and microtopics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_topics = pd.concat([right_topics[['MacroTopic', 'TopicName']], left_topics[['MacroTopic', 'TopicName']]], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We then create new columns in the dataset with the macro topics for each micro topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_macro_topics= get_macro_topic(baseline,macro_topics)\n",
    "finetuned_macro_topics= get_macro_topic(finetuned,macro_topics)\n",
    "baseline_b_macro_topics= get_macro_topic(baseline_b,macro_topics,flag= True)\n",
    "finetuned_b_macro_topics= get_macro_topic(finetuned_b,macro_topics)\n",
    "\n",
    "baseline['MacroTopic']= baseline_macro_topics\n",
    "finetuned['MacroTopic']= finetuned_macro_topics\n",
    "baseline_b['MacroTopic']= baseline_b_macro_topics\n",
    "finetuned_b['MacroTopic']= finetuned_b_macro_topics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see if we are able to predict the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_copy = baseline.copy()\n",
    "finetuned_copy = finetuned.copy()\n",
    "baseline_b_copy = baseline_b.copy()\n",
    "finetuned_b_copy = finetuned_b.copy()\n",
    "\n",
    "train_data=pd.read_csv('politicians_data/fine_tune_data.csv')\n",
    "train_copy = train_data.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We now apply category mappings on the train, baseline and finetuned datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Governance e posizionamento politico', 1: 'Governance e relazioni internazionali', 2: 'Politica economica e sociale', 3: 'Questioni economiche e di sviluppo', 4: 'Relazioni internazionali e valori progressisti', 5: 'Valori nazionali e questioni sociali'}\n"
     ]
    }
   ],
   "source": [
    "train_copy['Macro_Topic'] = train_copy['Macro_Topic'].astype('category')\n",
    "\n",
    "train_category_mapping = dict(enumerate(train_copy['Macro_Topic'].cat.categories))\n",
    "print(train_category_mapping)\n",
    "train_reverse_mapping = {v: k for k, v in train_category_mapping.items()}\n",
    "\n",
    "train_copy['topic_mapping'] = train_copy['Macro_Topic'].map(train_reverse_mapping).astype('Int64')\n",
    "\n",
    "finetuned_copy['topic_mapping'] = finetuned_copy['MacroTopic'].map(train_reverse_mapping)\n",
    "\n",
    "finetuned_b_copy['topic_mapping'] = finetuned_b_copy['MacroTopic'].map(train_reverse_mapping)\n",
    "\n",
    "baseline_copy['topic_mapping'] = baseline_copy['MacroTopic'].map(train_reverse_mapping)\n",
    "\n",
    "baseline_b_copy['topic_mapping'] = baseline_b_copy['MacroTopic'].map(train_reverse_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We then rename some columns for practicality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy.rename(columns={'Content': 'Tweets'}, inplace=True)\n",
    "finetuned_copy.rename(columns={'generated_tweet': 'Tweets','party': 'Party','tone': 'Sentiment'}, inplace=True)\n",
    "baseline_copy.rename(columns={'generated_tweet': 'Tweets','party': 'Party','tone': 'Sentiment'}, inplace=True)\n",
    "baseline_b_copy.rename(columns={'generated_tweets': 'Tweets','party': 'Party','tone': 'Sentiment'}, inplace=True)\n",
    "finetuned_b_copy.rename(columns={'generated_tweets': 'Tweets','party': 'Party','tone': 'Sentiment'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We then clean the party and sentiment column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_parties= baseline_copy['Party']\n",
    "f_parties= finetuned_copy['Party']\n",
    "\n",
    "b_parties_b= baseline_b_copy['Party']\n",
    "f_parties_b= finetuned_b_copy['Party']\n",
    "\n",
    "new_base_parties= clean_party(b_parties)\n",
    "new_finetuned_parties= clean_party(f_parties)\n",
    "new_base_parties_b= clean_party(b_parties_b)\n",
    "new_finetuned_parties_b= clean_party(f_parties_b)\n",
    "\n",
    "baseline_copy['Party']= new_base_parties\n",
    "finetuned_copy['Party']= new_finetuned_parties\n",
    "baseline_b_copy['Party']= new_base_parties_b\n",
    "finetuned_b_copy['Party']= new_finetuned_parties_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_sentiments= baseline_copy['Sentiment']\n",
    "f_sentiments= finetuned_copy['Sentiment']\n",
    "b_sentiments_b= baseline_b_copy['Sentiment']\n",
    "f_sentiments_b= finetuned_b_copy['Sentiment']\n",
    "\n",
    "new_base_sentiments= clean_sentiment(b_sentiments)\n",
    "new_finetuned_sentiments= clean_sentiment(f_sentiments)\n",
    "new_base_sentiments_b= clean_sentiment(b_sentiments_b)\n",
    "new_finetuned_sentiments_b= clean_sentiment(f_sentiments_b)\n",
    "\n",
    "baseline_copy['Sentiment']= new_base_sentiments\n",
    "finetuned_copy['Sentiment']= new_finetuned_sentiments\n",
    "baseline_b_copy['Sentiment']= new_base_sentiments_b\n",
    "finetuned_b_copy['Sentiment']= new_finetuned_sentiments_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We also apply category mapping to both columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Critico / Negativo', 1: 'Esortativo / Propaganda', 2: 'Generico', 3: 'Neutro / Informativo', 4: 'Supporto / Positivo'}\n",
      "{0: 'Destra', 1: 'Sinistra'}\n"
     ]
    }
   ],
   "source": [
    "train_copy['Sentiment'] = train_copy['Sentiment'].astype('category')\n",
    "sentiment_mapping = dict(enumerate(train_copy['Sentiment'].cat.categories))\n",
    "train_copy['Party'] = train_copy['Party'].astype('category')\n",
    "print(sentiment_mapping)\n",
    "reverse_sent_mapping = {v: k for k, v in sentiment_mapping.items()}\n",
    "\n",
    "\n",
    "party_mapping = dict(enumerate(train_copy['Party'].cat.categories))\n",
    "print(party_mapping)\n",
    "reverse_party_mapping= {v: k for k, v in party_mapping.items()}\n",
    "\n",
    "\n",
    "#apply the mapping to the sentiment and party column of finetuned and baseline\n",
    "finetuned_copy['sentiment_mapping'] = finetuned_copy['Sentiment'].map(reverse_sent_mapping).astype('Int64')\n",
    "baseline_copy['sentiment_mapping'] = baseline_copy['Sentiment'].map(reverse_sent_mapping).astype('Int64')\n",
    "\n",
    "baseline_b_copy['party_mapping'] = baseline_b_copy['Party'].map(reverse_party_mapping).astype('Int64')\n",
    "finetuned_b_copy['party_mapping'] = finetuned_b_copy['Party'].map(reverse_party_mapping).astype('Int64')\n",
    "\n",
    "baseline_b_copy['sentiment_mapping'] = baseline_b_copy['Sentiment'].map(reverse_sent_mapping).astype('Int64')\n",
    "finetuned_b_copy['sentiment_mapping'] = finetuned_b_copy['Sentiment'].map(reverse_sent_mapping).astype('Int64')\n",
    "\n",
    "baseline_copy['party_mapping'] = baseline_copy['Party'].map(reverse_party_mapping).astype('Int64')\n",
    "finetuned_copy['party_mapping'] = finetuned_copy['Party'].map(reverse_party_mapping).astype('Int64')\n",
    "\n",
    "train_copy['party_mapping'] = train_copy['Party'].map(reverse_party_mapping).astype('Int64')\n",
    "train_copy['sentiment_mapping'] = train_copy['Sentiment'].map(reverse_sent_mapping).astype('Int64')\n",
    "train_copy.drop(columns=['Sentiment','Party'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We then preprocess training tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tweets: 100%|██████████| 17241/17241 [02:59<00:00, 96.29it/s] \n"
     ]
    }
   ],
   "source": [
    "generated_tweets = train_copy['Tweets']\n",
    "tqdm.pandas(desc=\"Processing tweets\")\n",
    "train_copy['processed_tweet'] = generated_tweets.progress_apply(lambda x: preprocess_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy['processed_tweet'] = train_copy['processed_tweet'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_target= baseline_copy['topic_mapping']\n",
    "finetuned_target= finetuned_copy['topic_mapping']\n",
    "baseline_b_target= baseline_b_copy['topic_mapping']\n",
    "finetuned_b_target= finetuned_b_copy['topic_mapping']\n",
    "train_target= train_copy['topic_mapping']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy.drop(columns=['Dominant_Topic', 'Topic_Words', 'Macro_Topic', 'topic_mapping', 'Tweets'], inplace=True)\n",
    "\n",
    "baseline_copy.drop(columns=['original_tweet', 'topic', 'Sentiment', 'Party', 'topic_mapping', 'MacroTopic'], inplace=True)\n",
    "\n",
    "baseline_b_copy.drop(columns=['original_tweets', 'topic', 'Sentiment', 'Party', 'topic_mapping', 'MacroTopic'], inplace=True)\n",
    "\n",
    "finetuned_b_copy.drop(columns=['original_tweets', 'topic', 'Sentiment', 'Party', 'topic_mapping', 'MacroTopic'], inplace=True)\n",
    "\n",
    "finetuned_copy.drop(columns=['original_tweet', 'topic', 'Sentiment', 'Party', 'topic_mapping', 'MacroTopic'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We then preprocess test tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing baseline tweets: 100%|██████████| 3449/3449 [00:45<00:00, 75.09it/s]\n",
      "Processing finetuned tweets: 100%|██████████| 3449/3449 [00:39<00:00, 87.84it/s]\n",
      "Processing baseline_b tweets: 100%|██████████| 3449/3449 [00:57<00:00, 59.74it/s]\n",
      "Processing finetuned_b tweets: 100%|██████████| 3449/3449 [00:41<00:00, 82.25it/s]\n"
     ]
    }
   ],
   "source": [
    "baseline_tweets= baseline_copy['Tweets']\n",
    "baseline_b_tweets= baseline_b_copy['Tweets']\n",
    "finetuned_tweets= finetuned_copy['Tweets']\n",
    "finetuned_b_tweets= finetuned_b_copy['Tweets']\n",
    "\n",
    "tqdm.pandas(desc=\"Processing baseline tweets\")\n",
    "baseline_copy['processed_tweet'] = baseline_tweets.progress_apply(lambda x: preprocess_text(x))\n",
    "\n",
    "tqdm.pandas(desc=\"Processing finetuned tweets\")\n",
    "finetuned_copy['processed_tweet'] = finetuned_tweets.progress_apply(lambda x: preprocess_text(x))\n",
    "\n",
    "tqdm.pandas(desc=\"Processing baseline_b tweets\")\n",
    "baseline_b_copy['processed_tweet'] = baseline_b_tweets.progress_apply(lambda x: preprocess_text(x))\n",
    "\n",
    "tqdm.pandas(desc=\"Processing finetuned_b tweets\")\n",
    "finetuned_b_copy['processed_tweet'] = finetuned_b_tweets.progress_apply(lambda x: preprocess_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_copy.drop(columns=['Tweets'], inplace=True)\n",
    "baseline_b_copy.drop(columns=['Tweets'], inplace=True)\n",
    "finetuned_b_copy.drop(columns=['Tweets'], inplace=True)\n",
    "finetuned_copy.drop(columns=['Tweets'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_copy['processed_tweet'] = baseline_copy['processed_tweet'].apply(lambda x: ' '.join(x))\n",
    "finetuned_copy['processed_tweet'] = finetuned_copy['processed_tweet'].apply(lambda x: ' '.join(x))\n",
    "baseline_b_copy['processed_tweet'] = baseline_b_copy['processed_tweet'].apply(lambda x: ' '.join(x))\n",
    "finetuned_b_copy['processed_tweet'] = finetuned_b_copy['processed_tweet'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We finally split all the datasets into right and left wings to be prepared for the classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_wing_train = train_copy[train_copy['party_mapping'] == 1]\n",
    "right_wing_train = train_copy[train_copy['party_mapping'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_wing_train.drop(columns=['party_mapping'], inplace=True)\n",
    "right_wing_train.drop(columns=['party_mapping'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target_right = train_target[train_target.isin([1, 3, 5])]\n",
    "train_target_left = train_target[train_target.isin([0, 2, 4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_b_copy_right = baseline_b_copy[baseline_b_copy['party_mapping'] == 0]\n",
    "baseline_b_copy_right.drop(columns=['party_mapping'], inplace=True)\n",
    "baseline_b_copy_left = baseline_b_copy[baseline_b_copy['party_mapping'] == 1]\n",
    "baseline_b_copy_left.drop(columns=['party_mapping'], inplace=True)\n",
    "finetuned_b_copy_right = finetuned_b_copy[finetuned_b_copy['party_mapping'] == 0]\n",
    "finetuned_b_copy_right.drop(columns=['party_mapping'], inplace=True)\n",
    "finetuned_b_copy_left = finetuned_b_copy[finetuned_b_copy['party_mapping'] == 1]\n",
    "finetuned_b_copy_left.drop(columns=['party_mapping'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_b_target_right = baseline_b_target[baseline_b_target.isin([1, 3, 5])]\n",
    "baseline_b_target_left = baseline_b_target[baseline_b_target.isin([0, 2, 4])]\n",
    "finetuned_b_target_right = finetuned_b_target[finetuned_b_target.isin([1, 3, 5])]\n",
    "finetuned_b_target_left = finetuned_b_target[finetuned_b_target.isin([0, 2, 4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_copy_right= baseline_copy[baseline_copy['party_mapping'] == 0]\n",
    "baseline_copy_right.drop(columns=['party_mapping'], inplace=True)\n",
    "baseline_copy_left= baseline_copy[baseline_copy['party_mapping'] == 1]\n",
    "baseline_copy_left.drop(columns=['party_mapping'], inplace=True)\n",
    "finetuned_copy_right= finetuned_copy[finetuned_copy['party_mapping'] == 0]\n",
    "finetuned_copy_right.drop(columns=['party_mapping'], inplace=True)\n",
    "finetuned_copy_left= finetuned_copy[finetuned_copy['party_mapping'] == 1]\n",
    "finetuned_copy_left.drop(columns=['party_mapping'], inplace=True)\n",
    "baseline_target_right = baseline_target[baseline_target.isin([1, 3, 5])]\n",
    "baseline_target_left = baseline_target[baseline_target.isin([0, 2, 4])]\n",
    "finetuned_target_right = finetuned_target[finetuned_target.isin([1, 3, 5])]\n",
    "finetuned_target_left = finetuned_target[finetuned_target.isin([0, 2, 4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_copy_right.to_csv('files/baseline/baseline_right.csv', index=False)\n",
    "baseline_copy_left.to_csv('files/baseline/baseline_left.csv', index=False)\n",
    "baseline_target_right.to_csv('files/baseline/baseline_target_right.csv', index=False)\n",
    "baseline_target_left.to_csv('files/baseline/baseline_target_left.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_b_copy_right.to_csv('files/baseline_b/baseline_b_right.csv', index=False)\n",
    "baseline_b_copy_left.to_csv('files/baseline_b/baseline_b_left.csv', index=False)\n",
    "baseline_b_target_right.to_csv('files/baseline_b/baseline_b_target_right.csv', index=False)\n",
    "baseline_b_target_left.to_csv('files/baseline_b/baseline_b_target_left.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_copy_right.to_csv('files/finetuned/finetuned_right.csv', index=False)\n",
    "finetuned_copy_left.to_csv('files/finetuned/finetuned_left.csv', index=False)\n",
    "finetuned_target_right.to_csv('files/finetuned/finetuned_target_right.csv', index=False)\n",
    "finetuned_target_left.to_csv('files/finetuned/finetuned_target_left.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_b_copy_right.to_csv('files/finetuned_b/finetuned_b_right.csv', index=False)\n",
    "finetuned_b_copy_left.to_csv('files/finetuned_b/finetuned_b_left.csv', index=False)\n",
    "finetuned_b_target_right.to_csv('files/finetuned_b/finetuned_b_target_right.csv', index=False)\n",
    "finetuned_b_target_left.to_csv('files/finetuned_b/finetuned_b_target_left.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_wing_train.to_csv('files/train/train_right.csv', index=False)\n",
    "left_wing_train.to_csv('files/train/train_left.csv', index=False)\n",
    "train_target_right.to_csv('files/train/train_target_right.csv', index=False)\n",
    "train_target_left.to_csv('files/train/train_target_left.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
